# ğŸ“Œ HÆ¯á»šNG DáºªN CHáº Y INTERNVL (INFERENCE & TRAINING)

---

## âœ… 1. CÃ i Ä‘áº·t mÃ´i trÆ°á»ng
```bash
git clone https://github.com/OpenGVLab/InternVL.git
cd InternVL
conda create -n internvl python=3.9 -y
conda activate internvl
pip install -r requirements.txt
```

---
## âœ… 2. CÃ i phá»¥ thuá»™c má»Ÿ rá»™ng
```bash
#option Ä‘á»ƒ chat or traning
pip install flash-attn==2.3.6 --no-build-isolation
#cÃ i Ä‘áº·t mmcv-full==1.6.2 cho tÃ¹y chá»n segmentation
pip install -U openmim
mim install mmcv-full==1.6.2
#cÃ i dáº·t apex ( tÃ¹y chá»n cho segmentation) 
git clone https://github.com/NVIDIA/apex.git
cd apex
git checkout 2386a91
pip install -v --no-build-isolation ./
```

---
## âœ… 3. Inference vá»›i InternVL 3.0
```bash
pip install -U huggingface_hub
cd pretrained/
huggingface-cli download --resume-download --local-dir-use-symlinks False OpenGVLab/InternVL3-1B --local-dir InternVL3-1B
```

---
## âœ… 4. Fine-tune InternVL 3.0 (Custom Dataset)
**BÆ°á»›c 1: metadata JSON**
```json
{
  "my_dataset": {
    "root": "data/my_dataset/images/",
    "annotation": "data/my_dataset/annotations.jsonl",
    "data_augment": false,
    "max_dynamic_patch": 12,
    "repeat_time": 1,
    "length": "number of samples in the dataset"
  }
}
```
**BÆ°á»›c 2: Fine-tune**
```bash
# Full (8 GPU)
GPUS=8 PER_DEVICE_BATCH_SIZE=1 sh shell/internvl2.0/2nd_finetune/internvl2_1b_qwen2_0_5b_dynamic_res_2nd_finetune_full.sh
# LoRA (2 GPU)
GPUS=2 sh shell/internvl2.0/2nd_finetune/internvl2_1b_qwen2_0_5b_dynamic_res_2nd_finetune_lora.sh
```

---
## âœ… 5. Fine-tune InternVL 1.0 (Stage 2)
**BÆ°á»›c 1: Dá»¯ liá»‡u COCO**
```bash
mkdir -p data/coco/annotations && cd data/coco
wget http://images.cocodataset.org/zips/{train2014,val2014,test2015}.zip && unzip '*.zip'
cd annotations
wget https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json
wget https://github.com/OpenGVLab/InternVL/releases/download/data/coco_karpathy_test.json

_______________________________________________________________________________________________
After the download is complete, the directory structure is:
data
â”œâ”€â”€ coco
â”‚   â”œâ”€â”€ annotations
â”‚   â”‚   â”œâ”€â”€ coco_karpathy_train.json
â”‚   â”œâ”€â”€ test2017
â”‚   â”œâ”€â”€ train2014
â”‚   â”œâ”€â”€ train2017
â”‚   â”œâ”€â”€ val2014
â”‚   â””â”€â”€ val2017
â”œâ”€â”€ flickr30k
â”‚   â”œâ”€â”€ flickr30k_cn_test.txt
â”‚   â”œâ”€â”€ flickr30k_cn_train.txt
â”‚   â”œâ”€â”€ flickr30k_test_karpathy.json
â”‚   â”œâ”€â”€ flickr30k_test_karpathy.txt
â”‚   â”œâ”€â”€ flickr30k_train_karpathy.txt
â”‚   â”œâ”€â”€ flickr30k_val_karpathy.txt
â”‚   â””â”€â”€ Images
â””â”€â”€ nocaps
    â”œâ”€â”€ images
    â””â”€â”€ nocaps_val_4500_captions.json
```
**BÆ°á»›c 2: ThÃªm custom dataset**
```python
ds_collections = {
  'my_dataset_coco_format': {
    'root': './data/my_dataset/',
    'annotation': './data/my_dataset/annotations.json',
  }
}
```
**BÆ°á»›c 3: Cháº¡y fine-tune**
| Loáº¡i | Lá»‡nh | YÃªu cáº§u GPU |
|------|------|-------------|
| Full | `sh shell/finetune/internvl_stage2_finetune_coco_364_bs1024_ep5.sh` | 32Ã—A100 (80G) |
| Head | `sh shell/head_finetune/internvl_stage2_finetune_coco_224_bs1024_ep5_head_4gpu.sh` | 4 GPU â‰¥32G |
| LoRA | `sh shell/lora_finetune/internvl_stage2_finetune_coco_224_bs1024_ep5_lora16_4gpu.sh` | 4 GPU â‰¥40G |

---
## âœ… 6. Inference / Evaluation
### Captioning
```bash
sh evaluate.sh pretrained/InternVL-14B-224px caption-coco
```
### Retrieval
```bash
cd clip_benchmark
CUDA_VISIBLE_DEVICES=0 python3 clip_benchmark/cli.py eval \
  --model_type internvl --language en --task zeroshot_retrieval \
  --dataset flickr30k --dataset_root ./data/flickr30k \
  --model internvl_c_retrieval_hf \
  --pretrained ./work_dirs/internvl_stage2_finetune_flickr_364_bs1024_ep10/ \
  --output result_ft.json
```

---
## ğŸ“š TÃ i nguyÃªn chÃ­nh thá»©c
- GitHub: https://github.com/OpenGVLab/InternVL
- HF models: https://huggingface.co/OpenGVLab
